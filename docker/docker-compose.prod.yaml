version: "3.9"

# Production-ready Airflow cluster configuration with CeleryExecutor
# 
# IMPORTANT: Before deploying to production:
# 1. Generate strong secrets using: openssl rand -base64 32
# 2. Set all environment variables in .env file
# 3. Configure SSL/TLS certificates
# 4. Review and adjust resource limits based on workload
# 5. Set up external monitoring and alerting
# 6. Configure automated backups
# 7. Review security policies and network isolation

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true
  db:
    driver: bridge
    internal: true

volumes:
  postgres-db-volume:
    driver: local
  postgres-backup:
    driver: local
  airflow_dags:
    driver: local
  airflow_logs:
    driver: local
  airflow_plugins:
    driver: local
  redis-data:
    driver: local
  nginx-certs:
    driver: local
  nginx-logs:
    driver: local

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
  airflow_fernet_key:
    file: ./secrets/airflow_fernet_key.txt
  airflow_secret_key:
    file: ./secrets/airflow_secret_key.txt
  airflow_admin_password:
    file: ./secrets/airflow_admin_password.txt
  gemini_api_key:
    file: ./secrets/gemini_api_key.txt
  pgadmin_password:
    file: ./secrets/pgadmin_password.txt

services:
  # ============= DATABASE LAYER =============
  postgres:
    image: ayushghimire95/postgre:1.0
    container_name: postgres_prod
    networks:
      - db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      POSTGRES_DB: airflow
      POSTGRES_INITDB_ARGS: "-E UTF8"
      PGDATA: /var/lib/postgresql/data/pgdata
    secrets:
      - postgres_password
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data/pgdata
      - postgres-backup:/backups
      - ./storage/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=2621kB
      -c min_wal_size=1GB
      -c max_wal_size=4GB

  pgadmin:
    image: dpage/pgadmin4:8.10
    container_name: pgadmin_prod
    networks:
      - db
      - backend
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@yourdomain.com}
      PGADMIN_DEFAULT_PASSWORD_FILE: /run/secrets/pgadmin_password
      PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION: 'True'
      PGADMIN_CONFIG_SESSION_COOKIE_SECURE: 'True'
      PGADMIN_CONFIG_SESSION_COOKIE_HTTPONLY: 'True'
      PGADMIN_CONFIG_SESSION_COOKIE_SAMESITE: 'Lax'
    secrets:
      - pgadmin_password
    volumes:
      - ./pgadmin/servers.json:/pgadmin4/servers.json:ro
      - ./pgadmin/pgpass:/pgadmin4/pgpass:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy

  # ============= CACHE LAYER =============
  redis:
    image: redis:7.2-alpine
    container_name: redis_prod
    networks:
      - backend
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
      --appendfsync everysec
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============= AIRFLOW INIT =============
  airflow-init:
    image: apache/airflow:2.9.1
    container_name: airflow_init
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY_FILE: /run/secrets/airflow_fernet_key
      AIRFLOW__WEBSERVER__SECRET_KEY_FILE: /run/secrets/airflow_secret_key
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USER:-admin}
      _AIRFLOW_WWW_USER_PASSWORD_FILE: /run/secrets/airflow_admin_password
    secrets:
      - airflow_fernet_key
      - airflow_secret_key
      - airflow_admin_password
    volumes:
      - ./entrypoint.sh:/entrypoint.sh:ro
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      
    entrypoint: /bin/bash
    command: >
      -c "
      airflow db migrate &&
      airflow users create --role Admin --username $${_AIRFLOW_WWW_USER_USERNAME} --email admin@example.com --firstname Admin --lastname User --password $$(cat /run/secrets/airflow_admin_password) || true
      "
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: "no"

  # ============= AIRFLOW SERVICES =============
  airflow-webserver:
    image: apache/airflow:2.9.1
    container_name: airflow_webserver
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY_FILE: /run/secrets/airflow_fernet_key
      AIRFLOW__WEBSERVER__SECRET_KEY_FILE: /run/secrets/airflow_secret_key
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'false'
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
      AIRFLOW__CORE__CHECK_SLAS: 'true'
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__WEBSERVER__WORKERS: 4
      AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL: 30
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 3
    secrets:
      - airflow_fernet_key
      - airflow_secret_key
    volumes:
      - ./entrypoint.sh:/entrypoint.sh:ro
      - airflow_dags:/opt/airflow/dags:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins:ro
      
  
    expose:
      - 8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    image: apache/airflow:2.9.1
    container_name: airflow_scheduler
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY_FILE: /run/secrets/airflow_fernet_key
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 512
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 300
    secrets:
      - airflow_fernet_key
    volumes:
      - ./entrypoint.sh:/entrypoint.sh:ro
      - airflow_logs:/opt/airflow/logs:ro
      - airflow_dags:/opt/airflow/dags:ro
      - airflow_plugins:/opt/airflow/plugins:ro
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    image: apache/airflow:2.9.1
    container_name: airflow_worker
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY_FILE: /run/secrets/airflow_fernet_key
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 16
    secrets:
      - airflow_fernet_key
    volumes:
      - ./entrypoint.sh:/entrypoint.sh:ro
      - airflow_dags:/opt/airflow/dags:ro
      - airflow_logs:/opt/airflow/logs:ro
      - airflow_plugins:/opt/airflow/plugins:ro
      
    command: celery worker
    healthcheck:
      test: ["CMD-SHELL", "celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d celery@$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-flower:
    image: apache/airflow:2.9.1
    container_name: airflow_flower
    networks:
      - backend
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD}@postgres/airflow
      AIRFLOW__CELERY__FLOWER_BASIC_AUTH: ${FLOWER_USER:-admin}:${FLOWER_PASSWORD}
    command: celery flower
    expose:
      - 5555
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  # ============= APPLICATION SERVICES =============
  api:
    image: ayushghimire95/fastapi:1.
    container_name: fastapi_prod
    networks:
      - backend
      - db
    environment:
      DB_NAME: airflow
      DB_USER: airflow
      DB_PASSWORD: airflow
      DB_HOST: postgres
      DB_PORT: 5432
      GEMINI_API_KEY_FILE: /run/secrets/gemini_api_key
      DATABASE: articles
      WORKERS: 4
      LOG_LEVEL: info
    secrets:
      - gemini_api_key
    expose:
      - 8000
    command: ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4", "--log-level", "info"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy

  streamlit:
    image: ayushghimire95/streamlit:1.0
    container_name: streamlit_prod
    networks:
      - backend
      - db
    environment:
      DB_NAME: airflow
      DB_USER: airflow
      DB_PASSWORD: airflow
      DB_HOST: postgres
      DB_PORT: 5432
    expose:
      - 8501
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./dashboard:/src/dashboard:ro
    depends_on:
      api:
        condition: service_healthy

  # ============= REVERSE PROXY & LOAD BALANCER =============
  nginx:
    image: nginx:1.25-alpine
    container_name: nginx_prod
    networks:
      - frontend
      - backend
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - nginx-certs:/etc/nginx/certs:ro
      - nginx-logs:/var/log/nginx
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      - api
      - streamlit
      - airflow-webserver

  # ============= MONITORING =============
  