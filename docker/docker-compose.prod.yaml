version: "3.9"

# Production-ready Airflow cluster configuration with CeleryExecutor

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true
  db:
    driver: bridge
    internal: true

volumes:
  postgres-db-volume:
    driver: local
  postgres-backup:
    driver: local
  airflow_dags:
    driver: local
  airflow_logs:
    driver: local
  airflow_plugins:
    driver: local
  redis-data:
    driver: local
  nginx-certs:
    driver: local
  nginx-logs:
    driver: local

services:
  # ============= DATABASE LAYER =============
  postgres:
    image: ayushghimire95/postgre:1.0
    container_name: postgres_prod
    networks:
      - db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow123
      POSTGRES_DB: airflow
      POSTGRES_INITDB_ARGS: "-E UTF8"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data/pgdata
      - postgres-backup:/backups
      - ./storage/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=2621kB
      -c min_wal_size=1GB
      -c max_wal_size=4GB

  pgadmin:
    image: dpage/pgadmin4:8.10
    container_name: pgadmin_prod
    networks:
      - db
      - backend
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@yourdomain.com
      PGADMIN_DEFAULT_PASSWORD: admin123
      PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION: 'True'
      PGADMIN_CONFIG_SESSION_COOKIE_SECURE: 'True'
      PGADMIN_CONFIG_SESSION_COOKIE_HTTPONLY: 'True'
      PGADMIN_CONFIG_SESSION_COOKIE_SAMESITE: 'Lax'
    volumes:
      - ./pgadmin/servers.json:/pgadmin4/servers.json:ro
      - ./pgadmin/pgpass:/pgadmin4/pgpass:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy

  # ============= CACHE LAYER =============
  redis:
    image: redis:7.2-alpine
    container_name: redis_prod
    networks:
      - backend
    command: >
      redis-server
      --requirepass redis123
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
      --appendfsync everysec
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============= AIRFLOW INIT =============
  airflow-init:
    image: apache/airflow:2.9.1
    container_name: airflow_init
    user: "0:0"
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow123@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: d7I2QzTuWsVmCKqVJADzxrDf1A5QvK-lP6M7nXyZaBQ=
      AIRFLOW__WEBSERVER__SECRET_KEY: airflow-secret-key-change-me
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin123
      _AIRFLOW_WWW_USER_EMAIL: admin@example.com
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    command: >
      bash -c "
      mkdir -p /opt/airflow/logs/scheduler &&
      mkdir -p /opt/airflow/logs/dag_processor_manager &&
      mkdir -p /opt/airflow/dags &&
      mkdir -p /opt/airflow/plugins &&
      chown -R 50000:50000 /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins &&
      chmod -R 775 /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins &&
      airflow db migrate &&
      airflow users create --role Admin --username admin --email admin@example.com --firstname Admin --lastname User --password admin123 || true
      "
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: "no"

  # ============= AIRFLOW SERVICES =============
  airflow-webserver:
    image: apache/airflow:2.9.1
    container_name: airflow_webserver
    user: "50000:50000"
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:redis123@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: d7I2QzTuWsVmCKqVJADzxrDf1A5QvK-lP6M7nXyZaBQ=
      AIRFLOW__WEBSERVER__SECRET_KEY: airflow-secret-key-change-me
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'false'
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
      AIRFLOW__CORE__CHECK_SLAS: 'true'
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__WEBSERVER__WORKERS: 4
      AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL: 30
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 3
    volumes:
      - airflow_dags:/opt/airflow/dags:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins:ro
    command: airflow webserver
    expose:
      - 8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    image: apache/airflow:2.9.1
    container_name: airflow_scheduler
    user: "50000:50000"
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:redis123@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: d7I2QzTuWsVmCKqVJADzxrDf1A5QvK-lP6M7nXyZaBQ=
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 512
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 300
    volumes:
      - airflow_logs:/opt/airflow/logs
      - airflow_dags:/opt/airflow/dags:ro
      - airflow_plugins:/opt/airflow/plugins:ro
    command: airflow scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    image: apache/airflow:2.9.1
    container_name: airflow_worker
    user: "50000:50000"
    networks:
      - backend
      - db
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:redis123@redis:6379/0
      AIRFLOW__CORE__FERNET_KEY: d7I2QzTuWsVmCKqVJADzxrDf1A5QvK-lP6M7nXyZaBQ=
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 16
    volumes:
      - airflow_dags:/opt/airflow/dags:ro
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins:ro
    command: celery worker
    healthcheck:
      test: ["CMD-SHELL", "celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d celery@$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-flower:
    image: apache/airflow:2.9.1
    container_name: airflow_flower
    user: "50000:50000"
    networks:
      - backend
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://:redis123@redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow123@postgres/airflow
      AIRFLOW__CELERY__FLOWER_BASIC_AUTH: admin:admin123
    command: celery flower
    expose:
      - 5555
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      redis:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  # ============= APPLICATION SERVICES =============
  api:
    image: ayushghimire95/fastapi:1.4
    container_name: fastapi_prod
    networks:
      - backend
      - db
    environment:
      DB_NAME: airflow
      DB_USER: airflow
      DB_PASSWORD: airflow123
      DB_HOST: postgres
      DB_PORT: 5432
      GEMINI_API_KEY: your-gemini-api-key-here
      DATABASE: articles
      WORKERS: 4
      LOG_LEVEL: info
    expose:
      - 8000
    command: ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4", "--log-level", "info"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      postgres:
        condition: service_healthy

  streamlit:
    image: ayushghimire95/streamlit:1.0
    container_name: streamlit_prod
    networks:
      - backend
      - db
    environment:
      DB_NAME: airflow
      DB_USER: airflow
      DB_PASSWORD: airflow123
      DB_HOST: postgres
      DB_PORT: 5432
    expose:
      - 8501
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./dashboard:/src/dashboard:ro
    depends_on:
      api:
        condition: service_healthy

  # ============= REVERSE PROXY & LOAD BALANCER =============
  nginx:
    image: nginx:1.25-alpine
    container_name: nginx_prod
    networks:
      - frontend
      - backend
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - nginx-certs:/etc/nginx/certs:ro
      - nginx-logs:/var/log/nginx
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    depends_on:
      - api
      - streamlit
      - airflow-webserver